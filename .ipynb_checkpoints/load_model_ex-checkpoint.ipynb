{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import matplotlib.pyplot as plt\n",
    "ts = TimeSeries(key='B53N03ODVZVOH8R3',output_format='pandas')\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr = 'TGT'\n",
    "skipTrain = True\n",
    "modelName = \"tgtModel.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(abbr):\n",
    "    name = str(abbr + '_data.csv')\n",
    "    # Use the Alpha Vantage API to get Target stock data\n",
    "    data, metadata=ts.get_daily(abbr,outputsize='full')\n",
    "    data.to_csv(name)\n",
    "    data['date'] = data.index\n",
    "    return data, name, abbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadScale(name):\n",
    "    training_complete = pd.read_csv(name)\n",
    "    training_processed = training_complete.iloc[:, 1:2].values\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    training_scaled = scaler.fit_transform(training_processed)\n",
    "    return training_complete, training_processed, scaler, training_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(training_scaled, abbr):\n",
    "    if skipTrain == True:\n",
    "        model = load_model(modelName)\n",
    "    elif skipTrain == False:\n",
    "        features_set = []\n",
    "        labels = []\n",
    "        for i in range(60, 5226):\n",
    "            features_set.append(training_scaled[i-60:i, 0])\n",
    "            labels.append(training_scaled[i, 0])\n",
    "\n",
    "        # Convert both the feature_set and the labels list to the numpy array before we can use it for training\n",
    "        features_set, labels = np.array(features_set), np.array(labels)\n",
    "\n",
    "        # In order to train LSTM on our data, we need to convert our data into the shape accepted by the LSTM.\n",
    "        # We need to convert our data into three-dimensional format\n",
    "        features_set = np.reshape(features_set, (features_set.shape[0], features_set.shape[1], 1))\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=50, return_sequences=True, input_shape=(features_set.shape[1], 1)))\n",
    "\n",
    "        # Add dropout layer to avoid overfitting of the data\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units=50, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(LSTM(units=50, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(LSTM(units=50))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # To make our model more robust we add a dense layer\n",
    "        model.add(Dense(units = 1))\n",
    "\n",
    "        model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "        # Train the model\n",
    "\n",
    "        model.fit(features_set, labels, epochs = 100, batch_size = 32)\n",
    "        \n",
    "        # Save Model\n",
    "        modelTarget = str(abbr + 'Model.h5')\n",
    "        model.save(modelTarget)\n",
    "\n",
    "    # Pull in recent stock data to test the prediction model against - last 100 days of data\n",
    "    data2, metadata=ts.get_daily(abbr,outputsize='compact')\n",
    "    name = str(abbr + '_data_2.csv')\n",
    "    data2.to_csv(name)\n",
    "\n",
    "    return model, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(name, scaler, abbr, model, training_complete):\n",
    "    testing_complete = pd.read_csv(name)\n",
    "    name = str(abbr + '_predictions.png')\n",
    "    testing_processed = testing_complete.iloc[:, 1:2].values\n",
    "    total = pd.concat((training_complete['1. open'], testing_complete['1. open']), axis=0)\n",
    "    test_inputs = total[len(total) - len(testing_complete) - 60:].values\n",
    "    test_inputs = test_inputs.reshape(-1,1)\n",
    "    test_inputs = scaler.transform(test_inputs)\n",
    "    test_features = []\n",
    "    for i in range(60, 161):\n",
    "        test_features.append(test_inputs[i-60:i, 0])\n",
    "    test_features = np.array(test_features)\n",
    "    test_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], 1))\n",
    "    predictions = model.predict(test_features)\n",
    "    new_predictions = predictions[-1]\n",
    "    for i in range(0,10):\n",
    "        index = 61+i\n",
    "        for i in range(index, 161):\n",
    "            future_feature = test_inputs[101-i:161]\n",
    "    print(new_predictions)\n",
    "    print(\"------------------\")\n",
    "    print(future_feature)\n",
    "#     for i in range(0,10):\n",
    "#         index = 61+i\n",
    "#         for i in range(index, 161):\n",
    "#             future_feature = test_inputs[101-i:161]\n",
    "#         future_feature = np.array(future_feature)\n",
    "#         future_feature = np.reshape(future_feature, (future_feature.shape[0], future_feature.shape[1], 1))\n",
    "#         future_feature = np.concatenate((future_feature,new_predictions))\n",
    "#         new_pred = model.predict(future_feature)\n",
    "#         new_predictions.append(new_pred)\n",
    "#     predictions.append(new_predictions)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    print(predictions[100])\n",
    "    print(predictions[-1])\n",
    "    \n",
    "    # Plot the results -model trained with 100 epochs \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(testing_processed, color='blue', label='Actual Stock Price')\n",
    "    plt.plot(predictions , color='red', label='Predicted Stock Price')\n",
    "    plt.title('Stock Price Prediction')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(abbr):\n",
    "    data, name, abbr = getData(abbr)\n",
    "    training_complete, training_processed, scaler, training_scaled = loadScale(name)\n",
    "    model, name = trainModel(training_scaled, abbr)\n",
    "    predict(name, scaler, abbr, model, training_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runModel(abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(name, scaler, abbr, model, training_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
